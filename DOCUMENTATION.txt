END - TO - END MACHINE LEARNING PROJECT

-> DATA COLLECTION/INGESTION

-> DATA PREPROCESSING

-> EXPLORATORY DATA ANALYSIS/DATA VISUALIZATION

-> FEATURE ENGINEERING

-> MODEL SELECTION

-> MODEL TRAINING

-> MODEL EVALUATION

-> HYPER PARAMETER TUNING

-> DEPLOYEMENT
----------------------------------------------------------------------------------------------------------------------------------------

CREATING A VENV AND LINKING IT TO GITHUB REPOSITORY:

* Create a virtual environment(venv) in ML_PROJECT folder where all the required packages are installed.
	###conda create -p venv python==3.8 -y
	###conda activate venv/

{	vs code to github repository	}

//git init
*git add README.md
*git commit -m "first commit"
//git branch -M main
//git remote add origin https://github.com/SUREN7-7/ML_PROJECT.git
*git push -u origin main


{	github repository to vs code	}

*git pull

-----------------------------------------------------------------------------------------------------------------------------------------

creating 2 files setup.py and requirements.py
-> so setup.py is a python program which reads the data from text file ie., required packages and install them
	### pip install -r requirements.txt

The entire project implementation is from src.
creating components as a floder so that it can be used as a package and be utilized.
-> Developing a project initially begins with data ingestion which is a part of a module.
-> data transformation,data_trainer are represented as modules of the package components.

creating a pipeline
-> train_pipeline (which uses the components)
-> predict_pipeline

creating logger,exception,utils python files
logging: Logging is used to track events in a Python application. Instead of using print(), logging provides a better way to debug, monitor, 
	 and store runtime information.
exception: it contains UserDefinedException handling code
utils: It contains the code which can gather data from databases or deploy model in cloud

The sys.exc_info() {exection info}function provides details about the most recent exception that was caught in a try-except block. It returns a tuple with three values:
->Exception Type – The class of the exception (ZeroDivisionError, ValueError, etc.).
->Exception Value – The error message or details of the exception.
->Traceback Object – The traceback, which contains information about where the error occurred.
we actually need 3rd arg which represent the line of error, file

The __init__ method in Python is a special (dunder) method used to initialize an object when it is created. 
The __str__ method in Python is a special (dunder) method used to return a human-readable string representation of an object.

The os module in Python provides functions to interact with the operating system (Windows, Linux, macOS)
-> Work with files & directories
-> Execute system commands
-> Get environment variables
-> Manage processes

logging.debug()		Detailed debugging info								"Variable X has value Y"
logging.info()		General status updates								"Server started"
logging.warning()	Warnings that might cause issues					"Low disk space"
logging.error()		Errors that prevent normal execution				"File not found"
logging.critical()	Serious issues that require immediate attention		"System crash detected!"

check whether logger is working
#python src/logger.py
-----------------------------------------------------------------------------------------------------------------

											DATA INGESTION

This Python script is part of a machine learning pipeline. 
It handles data ingestion and data transformation, with placeholders for model training
DataIngestionConfig (Configuration Class):
	Defines paths where raw, training, and testing data will be stored.
	Uses os.path.join() to construct paths dynamically.
	Stored under the artifacts/ directory.
DataIngestion (Main Class):
	__init__(self)::
		Initializes an instance of DataIngestionConfig to store file paths.
	initiate_data_ingestion():
		This method is responsible for reading the dataset, splitting it into training and testing sets, and saving the files. 
	makedirs extract the folder artifacts and creates it if not exist 
	to_csv() creates a df to csv file and saves in the given path ie 1st arg
----------------------------------------------------------------------------------------------------------------------

											DATA TRANSFORMATION

This module is responsible for preprocessing raw data before it is used for model training. It handles:
	Handling missing values (imputation)
	Scaling numerical features
	Encoding categorical variables
	Saving the preprocessing object (preprocessor.pkl) for later use.
	A PKL file is a file saved in the Python pickle format, which contains serialized Python objects. 
	These files are typically used to store machine learning models, data pre-processing objects, or any Python objects
DataTransformationConfig (Configuration Class):
	Defines a configuration class to store the file path for the saved preprocessing object (proprocessor.pkl).
DataTransformation Class (Main Processing Class):
	__init__(self):
		This ensures that all methods inside this class can access the preprocessor.pkl file path
	get_data_transformer_object():
		This function creates and returns the preprocessing pipeline, which transforms numerical and categorical data.
	initiate_data_transformation(train_path, test_path):
		Reads the train and test CSV files.
		Prepares data for transformation.
		Applies preprocessing.
		Saves the preprocessing object (preprocessor.pkl).
		Returns processed training & testing arrays.
		path is obtained in data_ingestion
		Calls the get_data_transformer_object() function to get the preprocessor pipeline.

